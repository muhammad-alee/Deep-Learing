# -*- coding: utf-8 -*-
"""Car_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mvBKdtlGer0jRSj9YH0wmaXBPE-LssTp
"""

from google.colab import files 
files.upload()

!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d hellbuoy/car-price-prediction

!ls

!unzip car-price-prediction.zip

!ls

import pandas as pd
data = pd.read_csv('CarPrice_Assignment.csv')
data.head()

# show the boolean dataframe             
print(" \nshow the boolean Dataframe : \n\n", data.isnull()) 
  
# Count total NaN in a DataFrame 
print(" \nCount total NaN in a DataFrame : \n\n", 
       data.isnull().sum().sum())

import numpy as np
import pandas as pd
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense 
from tensorflow.keras.optimizers import Adam, SGD

print(tf.__version__)

data.head()

print(data.shape)

print(data.columns)

data.info()

from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
data['CarName'] = labelencoder.fit_transform(data['CarName'])
data['enginetype'] = labelencoder.fit_transform(data['enginetype'])
data['cylindernumber'] = labelencoder.fit_transform(data['cylindernumber'])
data['fueltype'] = labelencoder.fit_transform(data['fueltype'])
data['aspiration'] = labelencoder.fit_transform(data['aspiration'])
data['carbody'] = labelencoder.fit_transform(data['carbody'])
data['drivewheel'] = labelencoder.fit_transform(data['drivewheel'])
data['enginelocation'] = labelencoder.fit_transform(data['enginelocation'])
data['fuelsystem'] = labelencoder.fit_transform(data['enginelocation'])

data['doornumber'] = data['doornumber'].map({'two':2,'four':4})

data.head(10)

data.info()

X=data.iloc[:,0:25]
y=data.iloc[:,-1]

X.head()

y.head()

# separate dataset into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

corrmat = data.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap="RdYlGn")

# with the following function we can select highly correlated features
# it will remove the first feature that is correlated with anything other feature

def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

corr_features = correlation(X_train, 0.7)
len(set(corr_features))

corr_features

X_train = X_train.drop(corr_features,axis=1)
X_test= X_test.drop(corr_features,axis=1)

X_test

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler().fit(X_train)
X_train= scaler.transform(X_train)
X_test= scaler.transform(X_test)

# mean = x_train.mean(axis=0)
# x_train -= mean

# std = x_train.std(axis=0)

# x_train /= std
# x_test -= mean
# x_test /= std

# mean

# std

# np.mean(x_train, axis=0)

input_shape=(X_train.shape[1],)
input_shape

from tensorflow.keras import models
from tensorflow.keras import layers

def build_model():
    model = models.Sequential()
    model.add(layers.Dense(10, activation='relu',
                           input_shape=(X_train.shape[1],)))
    model.add(layers.Dense(8, activation='relu'))
    model.add(layers.Dense(6, activation='relu'))
    model.add(layers.Dense(1))
    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
    return model

model = build_model()
model.fit(X_train,y_train,validation_split=0.33,epochs=100,batch_size=16, verbose=0)
test_mse_score, test_mae_score = model.evaluate(X_test, y_test)

# predicting the test set results
y_pred = model.predict(X_test)

y_pred[:5].astype('int64')

y_test[:5]

# for i in range(k):
# print('processing fold #', i)
# val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
# val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
# partial_train_data = np.concatenate(
# [train_data[:i * num_val_samples],
# train_data[(i + 1) * num_val_samples:]],
# axis=0)
# partial_train_targets = np.concatenate(
# [train_targets[:i * num_val_samples],
# train_targets[(i + 1) * num_val_samples:]],
# axis=0)
# model = build_model()
# model.fit(partial_train_data, partial_train_targets,
# epochs=num_epochs, batch_size=1, verbose=0)
# val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
# all_scores.append(val_mae

# import numpy as np
# k=4
# num_val_samples = len(x_train) // k
# num_epochs = 100
# all_scores = []

# num_epochs = 500
# all_mae_histories = []
# for i in range(k):
#     print('processing fold #', i)
#     val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]
#     val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]
    
#     partial_train_data = np.concatenate(
#         [x_train[:i * num_val_samples],
#          x_train[(i + 1) * num_val_samples:]],
#         axis=0)
    
#     partial_train_targets = np.concatenate(
#         [y_train[:i * num_val_samples],
#          y_train[(i + 1) * num_val_samples:]],
#         axis=0)
    
# model = build_model()
# history = model.fit(partial_train_data, partial_train_targets,
#                     validation_data=(val_data, val_targets),
#                     epochs=num_epochs, batch_size=1, verbose=0)
# mae_history = history.history['val_mae']
# all_mae_histories.append(mae_history)

# history.history.keys()

# average_mae_history = [
# np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]

# import matplotlib.pyplot as plt
# plt.plot(average_mae_history)
# plt.xlabel('Epochs')
# plt.ylabel('Validation MAE')
# plt.show()

# import matplotlib.pyplot as plt
# plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
# plt.xlabel('Epochs')
# plt.ylabel('Validation MAE')
# plt.show()

# def smooth_curve(points, factor=0.9):
#     smoothed_points = []
#     for point in points:
#         if smoothed_points:
#             previous = smoothed_points[-1]
#             smoothed_points.append(previous * factor + point * (1 - factor))
#         else:
#             smoothed_points.append(point)
#     return smoothed_points
    
# smooth_mae_history = smooth_curve(average_mae_history[10:])
# plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)
# plt.xlabel('Epochs')
# plt.ylabel('Validation MAE')
# plt.show()

model = build_model()
model.fit(x_train, y_train,
          epochs=100, batch_size=16, verbose=0)
test_mse_score, test_mae_score = model.evaluate(x_test, y_test)

# predicting the test set results
y_pred = model.predict(x_test)
y_pred

y_pred[:5].astype('int64')

y_test[:5]

