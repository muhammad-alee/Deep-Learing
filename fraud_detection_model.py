# -*- coding: utf-8 -*-
"""Fraud Detection Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z3DbRWCRDgewJzjGvYLfSSREQ6L2ty6k

# Fraud Detection Model
In this tutorial I will walk you through one of the most important applications in machine learning, I will demonstrate how can construct a high-performance model to detect a credit card fraud by using deep learning model.
"""

import numpy as np
import pandas as pd
from sklearn.utils import shuffle
import matplotlib.pyplot as plt

# Import Keras, Dense, Dropout
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout

import seaborn as sns

"""# Dataset
The datasets contain transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days, where we have **492** frauds out of **284,807** transactions. The dataset is highly unbalanced, the positive class (frauds) account for **0.172%** of all transactions.

It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. **Features V1, V2, ... V28** are the principal components obtained with PCA, the only features which have not been transformed with PCA are **'Time' and 'Amount'**. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.

"""

from google.colab import files 
files.upload()

!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mlg-ulb/creditcardfraud

!ls

!unzip creditcardfraud.zip

!ls

"""## **Data Loading**"""

# Import the dataset
data = pd.read_csv('creditcard.csv')

# Print out first 5 row of the dataset
data.head(5)

# Count the number of samples for each class (In this case we have 2 classes)
data.Class.value_counts()

# show the boolean dataframe             
print(" \nshow the boolean Dataframe : \n\n", data.isnull()) 
  
# Count total NaN in a DataFrame 
print(" \nCount total NaN in a DataFrame : \n\n", 
       data.isnull().sum().sum())

pd.value_counts(data['Class']).plot.bar()

len(data.columns)

"""## **Splitting**"""

X=data.iloc[:,1:30]
y=data.iloc[:,-1]

X.head()

y.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)
y_train.value_counts()
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

print(len(X.columns))
n_inputs=29

weights_assigned={0:1,1:550}

input_shape=(X_train.shape[1])
input_shape

"""## **Network** **Building** **and** **Training**"""

from tensorflow.keras import models
from tensorflow.keras import layers

def build_model():
    model = models.Sequential()
    model.add(layers.Dense(10, activation='relu',
                           input_shape=(X_train.shape[1],), kernel_initializer='he_uniform'))
    model.add(layers.Dense(8, activation='relu'))
    model.add(layers.Dense(6, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

model = build_model()
history=model.fit(X_train,y_train,validation_split=0.33,class_weight=weights_assigned,epochs=100,batch_size=150, verbose=0)

y_pred=model.predict(X_test)

from sklearn.metrics import roc_auc_score
roc_auc_score(y_test,y_pred)

# list all data in history
print(history.history.keys())

"""## **Data** **Visualization**"""

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

"""## **Evaluation**"""

# Evaluation phase

# Use the testing set to evaluate the model
scores = model.evaluate(X_train,y_train)

# Print out the accuracy
print('\n')
print('Accuracy=', scores[1])

"""## **Prediction**"""

y_pred[:5].astype('int64')

y_test[:5]